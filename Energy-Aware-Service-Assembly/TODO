METEPOLITICA:

- Rank: metric  
- alfa: weight of history. 1=no history
- h: exploration parameter. h=infinity is greedy
- sng: lower is better (-) or higher is better (+)


POLITICHE:

Random: STRATEGY.equals("random")  
- Rank:   
- alfa: 
- h: 0
- sng:
- TODO: Nothing

Local: (STRATEGY.equals("local_energy_template"))
- Rank: L  
- alfa: variabile (1, 0.5)
- h: variabile --> h=infinito (politica greedy), h=costante (1? oppure 2?)
- sng: -

Overall: (STRATEGY.equals("overall_energy_template"))
- Rank: O 
- alfa: variabile (1, 0.5)
- h: variabile --> h=infinito (politica greedy), h=costante (1? oppure 2?)
- sng: -

Energy Aware (Learning):
- Testare al variare come le precedenti
- TODO: Riscriverla mettendo il suo ee. Riscriverla uguale a battery panel ma ee prevede solo il learning sul consumo

Residual Life:(STRATEGY.equals("weighted_random"))
- Rank: rl
- alfa: variabile come le altre
- h: variabile come le altre
- sng: piu

Energy Balance: (STRATEGY.equals("fair_energyBP"))
- Rank: ee
- alfa: variabile (1, 0.5)
- h: variabile --> h=infinito (politica greedy), h=costante (1? oppure 2?)
- sng: piu
- TODO: stare attenti a quale chiamare: P (panel) ,BP (battery panel)


SCENARIO E PARAMETRI:

RETE:
- Battery? Number of nodes?
- Ripartiamo dagli scenari statici della tesi di Francesca

Scenario A:
- nodi
- ecc
- ecc

Scenario B:
- nodi
- ecc
- ecc

Scenario C:
- nodi
- ecc
- ecc

TODO:
- Durante la sperimentatione bisogna controllare come si comporta il segno del template per le nostre politiche (-H ed H in application)
